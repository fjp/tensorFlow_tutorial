{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Tensorflow](https://www.tensorflow.org/) basics\n",
    "\n",
    "In this tutorial we are going to classify images from the notMNIST dataset . The goal is to automatically detect the letter based on the image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "# Create TensorFlow object called hello_constant\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "In TensorFlow data is not stored as strings, floats or strings. These values are encapsulated in an object called a [tensors](https://en.wikipedia.org/wiki/Tensor). In the case of `hello_constant = tf.constant('Hello World!')`, `hello_constant` is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A is a 0-dimensional int32 tensor\n",
    "A = tf.constant(1234)\n",
    "\n",
    "# B is a 1-dimensional in32 tensor\n",
    "B = tf.constant([123, 456, 789])\n",
    "\n",
    "# C is a 2-dimensional int 32 tensor\n",
    "C = tf.constant([ [123, 456, 789,], [222,333,444] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor returned by `tf.constant()` is called a constant tensor, because the value of the tensor never changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session\n",
    "\n",
    "TensorFlow’s api is built around the idea of a computational graph. The previous TensorFlow code can be turned into a graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TensorFlow_Session](./figures/session.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"TensorFlow Session\", as shown above, is an environment for running a graph. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code has already created the tensor, `hello_constant`, from the previous lines. The next step is to evaluate the tensor in a session.\n",
    "\n",
    "The code creates a session instance, `sess`, using `tf.Session`. The `sess.run()` function then evaluates the tensor and returns the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "\n",
    "If we want to use a non-constant we use [`tf.placeholder()`](https://www.tensorflow.org/api_docs/python/tf/placeholder) and `feed_dict`. Next we go over the basics of feeding data into TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.placeholder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [`tf.placeholder()`](https://www.tensorflow.org/api_docs/python/tf/placeholder) to use it as placeholder for arbitrary data input. Thus allowing TensorFlow to take in different datasets with different parameters.\n",
    "\n",
    "[`tf.placeholder()`](https://www.tensorflow.org/api_docs/python/tf/placeholder) returns a tensor that gets its value from data passed to the tf.session.run() function, allowing you to set the input right before the session runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session's feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `feed_dict` parameter in [`tf.session.run()`](https://www.tensorflow.org/api_docs/python/tf/Session#run) to set the placeholder tensor. The above example shows the tensor `x` being set to the string `\"Hello, world\"`. It's also possible to set more than one tensor using `feed_dict` as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If the data passed to the `feed_dict` doesn’t match the tensor type and can’t be cast into the tensor type, we get the error `“ValueError: invalid literal for`...”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Math\n",
    "\n",
    "After getting the input we are going to use it by applying basic math functions - add, subtract, multiply, and divide - with tensors. (There's many more math functions, see in the [documentation](https://www.tensorflow.org/api_docs/python/math_ops/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.add(5, 2)  # 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tf.add()`](https://www.tensorflow.org/api_guides/python/math_ops) function does exactly what you expect it to do. It takes in two numbers, two tensors, or one of each, and returns their sum as a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtraction and Multiplication\n",
    "\n",
    "Here’s an example with subtraction and multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.subtract(10, 4) # 6\n",
    "y = tf.multiply(2, 5)  # 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x tensor will evaluate to `6`, because `10 - 4 = 6`. The `y` tensor will evaluate to `10`, because `2 * 5 = 10`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting types\n",
    "\n",
    "It may be necessary to convert between types to make certain operators work together. For example, if we'd try the following, it would fail with an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tf.subtract(tf.constant(2.0),tf.constant(1))\n",
    "# Fails with ValueError: Tensor conversion requested dtype float32 \n",
    "# for Tensor with dtype int32:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's because the constant `1` is an integer but the constant `2.0` is a floating point value and subtract expects them to match.\n",
    "\n",
    "In cases like these, you can either make sure our data is all of the same type, or we can cast a value to another type. In this case, converting the `2.0` to an integer before subtracting, like so, will give the correct result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sub_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.cast(tf.divide(x,y), tf.int32),tf.constant(1))\n",
    "\n",
    "# Note:TensorFlow has multiple ways to divide.\n",
    "#   tf.divide(x,y) uses Python 3 division semantics and will return a float here\n",
    "#          It would be the best choice if all the other values had been floats\n",
    "#   tf.div(x,y) uses Python 2 division semantics and will return an integer here\n",
    "#          TensorFlow documentation suggests we should prefer tf.divide\n",
    "#   tf.floordiv(x,y) will do floating point division and then round down to the nearest\n",
    "#          integer (but the documentation says it may still represent\n",
    "#          its result as a floating point value)\n",
    "#   tf.cast(tf.divide(x,y), tf.int32)\n",
    "#          This lets us do floating point division and then cast it to an integer\n",
    "#          to match the 1 passed to subtract\n",
    "\n",
    "\n",
    "# TODO: Print z from a session\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "We did the following:\n",
    "- Ran operations in [`tf.Session`](https://www.tensorflow.org/api_docs/python/tf/Session).\n",
    "- Created a constant tensor with [`tf.constant()`](https://www.tensorflow.org/api_docs/python/tf/constant).\n",
    "- Used [`tf.placeholder()`](https://www.tensorflow.org/api_docs/python/tf/placeholder) and `feed_dict` to get input.\n",
    "- Applied the [`tf.add()`](https://www.tensorflow.org/api_docs/python/tf/add), [`tf.subtract()`](https://www.tensorflow.org/api_docs/python/tf/subtract), [`tf.multiply()`](https://www.tensorflow.org/api_docs/python/tf/multiply), and [`tf.divide()`](https://www.tensorflow.org/api_docs/python/tf/divide) functions using numeric data.\n",
    "- Learned about casting between types with [`tf.cast()`](https://www.tensorflow.org/api_docs/python/tf/cast)\n",
    "\n",
    "These are the basics of TensorFlow. Next we learn about one of the most popular applications of neural networks - classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification\n",
    "\n",
    "https://en.wikipedia.org/wiki/Statistical_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Logistic Classifier\n",
    "\n",
    "A logistic classifier takes an input e.g. the pixels of an image $X$ and applies a linear funciton to them to generate its predictions.\n",
    "\n",
    "$$\n",
    "WX + b = y\n",
    "$$\n",
    "\n",
    "$W$ are the weights and $b$ is the bias term. Output vector $y$ reflects the class of the input. This should be a probability vector where we want the probability of the correct class to be very close to one and the probability to every other class to be close to zero.\n",
    "The way to turn scores in to probabilities is to use a softmax function:\n",
    "\n",
    "$$\n",
    "S(y_i) = \\frac{e^{y_i}}{\\sum_j e^{y_j}}\n",
    "$$\n",
    "\n",
    "This function denoted by $S$ can turn any score into proper probabilities.\n",
    "Proper probabilities sum to one and they will be larger when the scores are large and small when the scores are comparatively smaller. Scores in the terms of logistic regression, are also often called logits.\n",
    "\n",
    "By training our network we are going to try to find the values for the weights and bias which are good at performing correct predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear functions in TensorFlow\n",
    "\n",
    "The most common operation in neural networks is calculating the linear combination of inputs, weights, and biases. As a reminder, we can write the output of the linear operation as\n",
    "\n",
    "$$\n",
    "y = xW + b\n",
    "$$\n",
    "\n",
    "Here, $W$ is a matrix of the weights connecting two layers. The output $y$, the input $x$, and the biases $b$ are all vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias in TensorFlow\n",
    "\n",
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, we'll need a Tensor that can be modified. This leaves out [`tf.placeholder()`](https://www.tensorflow.org/api_docs/python/tf/placeholder) and [`tf.constant()`](https://www.tensorflow.org/api_docs/python/tf/constant), since those Tensors can't be modified. This is where [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) class comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.Variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so we must initialize the state of the tensor manually. We'll use the [`tf.global_variables_initializer()`](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer) function to initialize the state of all the Variable tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tf.global_variables_initializer()`](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer) call returns an operation that will initialize all TensorFlow variables from the graph. We call the operation using a session to initialize all the variables as shown above. Using the [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "\n",
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time we train it. We'll see more about this in the next section, when we study gradient descent.\n",
    "\n",
    "Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. We'll use the [`tf.truncated_normal()`](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) function to generate random numbers from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.truncated_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tf.truncated_normal()`](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. Let's use the simplest solution, setting the bias to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.truncated_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tf.zeros()`](https://www.tensorflow.org/api_docs/python/tf/zeros) function returns a tensor with all zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifier Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![mnist](./figures/mnist-012.png)\n",
    "A subset of the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be classifying the handwritten numbers `0`, `1`, and `2` from the MNIST dataset using TensorFlow. The above is a small sample of the data we'll be training on. Notice how some of the `1`s are written with a [serif](https://en.wikipedia.org/wiki/Serif) at the top and at different angles. The similarities and differences will play a part in shaping the weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![weights](./figures/weights-0-1-2.png)\n",
    "Left: Weights for labeling 0. Middle: Weights for labeling 1. Right: Weights for labeling 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images above are trained weights for each label (`0`, `1`, and `2`). The weights display the unique properties of each digit they have found. In the following we will train our own weights using the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "Loss: 2.879290819168091\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('./mnist', one_hot=True)\n",
    "\n",
    "    # In order to make this run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "# get init operation that is used to initialize all TensorFlow variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # initialize all TensorFlow variables\n",
    "    session.run(init)\n",
    "\n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # We'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # We'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # We'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Softmax\n",
    "\n",
    "In the Intro to TFLearn we used the softmax function to calculate class probabilities as output from the network. The softmax function squashes it's inputs, typically called logits or logit scores, to be between `0` and `1` and also normalizes the outputs such that they all sum to `1`. This means the output of the softmax function is equivalent to a categorical probability distribution. It's the perfect function to use as the output activation for a network predicting multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figures/softmax-input-output.png)\n",
    "Example of the softmax function at work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Softmax\n",
    "We're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.nn.softmax([2.0, 1.0, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [`tf.nn.softmax()`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) implements the softmax function. It takes in logits and returns softmax activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65900117,  0.24243298,  0.09856589], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "We need a way to represent our labels mathematically. We want the probability for the correct class to be close to `1` and the probability for all the other classes close to `0`.\n",
    "Using [one-hot encoding](https://en.wikipedia.org/wiki/One-hot), each label will be represented by vector, that is as long as there are classes and it has a value `1.0` for the correct class and `0.0` every where else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding With Scikit-Learn\n",
    "\n",
    "Transforming labels into one-hot encoded vectors is pretty simple with scikit-learn using LabelBinarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Example labels\n",
    "labels = np.array([1,5,3,2,1,4,2,1,3])\n",
    "\n",
    "# Create the encoder\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Here the encoder finds the classes and assigns one-hot vectors \n",
    "lb.fit(labels)\n",
    "\n",
    "# And finally, transform the labels into one-hot encoded vectors\n",
    "lb.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in TensorFlow\n",
    "\n",
    "In the Intro to TFLearn lesson we discussed using cross entropy as the cost function for classification with one-hot encoded labels. Again, TensorFlow has a function to do the cross entropy calculations for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cross_entropy](./figures/cross-entropy-diagram.png)\n",
    "Cross entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a cross entropy function in TensorFlow, we'll need to use two new functions:\n",
    "\n",
    "- [`tf.reduce_sum()`](https://www.tensorflow.org/api_docs/python/tf/reduce_sum)\n",
    "- [`tf.log()`](https://www.tensorflow.org/api_docs/python/tf/log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.reduce_sum([1, 2, 3, 4, 5])  # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tf.reduce_sum()`](https://www.tensorflow.org/api_docs/python/tf/reduce_sum) function takes an array of numbers and sums them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.log(100.0)  # 4.60517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does exactly what you would expect it to do. [`tf.log()`](https://www.tensorflow.org/api_docs/python/tf/log) takes the natural log of a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# Calculate cross entropy\n",
    "cross_entropy = -tf.reduce_sum(tf.mul(one_hot, tf.log(softmax)))\n",
    "\n",
    "# Print cross entropy from session\n",
    "with tf.Session() as session:\n",
    "    output = session.run(cross_entropy, feed_dict={one_hot: one_hot_data, softmax: softmax_data})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Cross Entropy\n",
    "\n",
    "Having all the pieces of the puzzle, the quesion is how we are going to find the weights $w$ and the biases $b$ that will get our classifier to do what we want it to do. That is, have low distance for the correct class but have a high distance for the incorrect class. One thing we can do is to measure that distance averaged over the entire training set for all the inputs and the labels that we have available. That is called the training loss \n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_i = D(S(W x_i + b), y_i)\n",
    "$$\n",
    "\n",
    "This loss which is the average cross entropy over our entire training set is one humongous function. Every example in our training set gets multiplied by this one big matrix $W$ and then they get all added up in one big sum.\n",
    "We want all the distances to be small with, which would mean we are doing a good job at classifying every example in the training data. So we want the loss to be small. The loss is a function of the weights and the biases. So we are going to minimize that function.\n",
    "\n",
    "We can turn the machine learning problem into a numerical optimization problem and use gradient descent to find the minimium loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition into Practical Aspects of Learning\n",
    "\n",
    "In the next section we will see how we can use tensorFlow tools to compute the derivatives and see the pros and cons about gradient descent. But for now we assume that we have the optimizer as a black box that we can simply use. \n",
    "There are still two last practical things that stand in the way before we can train our model:\n",
    "\n",
    "1. How do we fill image pixels to this classifier?\n",
    "2. Where do we initialize the optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Stability\n",
    "\n",
    "When we do numerical computations, we always have to worry a bit about calculating values that are too large or too small. In particular, adding very small values, to a very large value can introduce a lot of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95367431640625\n"
     ]
    }
   ],
   "source": [
    "a = 1000000000\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "print(a - 1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add 1e-6 a million times to the value one million and after that subract again one million we should get 1.0 according to math. But the code result is 0.95 which is a big difference. \n",
    "\n",
    "If we replace the one billion with just one we see that the error becomes very tiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999177334\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "print(a - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Inputs and Initial Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of these numerical issues we want the values involved in the loss function to never get too big or too small.\n",
    "One good guiding prinicple is that we want our values to always have zero mean and equal variance whenever possible.\n",
    "On top of the numerical issues there are also a very good mathematical reasons to keep values we compute roughly around a mean of zero and equal variance when we are doing optimization (well conditioned). A badly conditioned problem means that the optimizer has to a lot of searching to go and find a good solution. A well conditioned problem makes it a lot easier for the optimizer to do its job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![normalization](./figures/normalization.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are dealing with images it is simple. We can take the pixel values of the images. They are typically between 0 and 255 and simply subtract 128 and divide by 128. This does not change the content of the image but it makes it much easier for the optmization to proceed numerically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want our weights and biases to be initialized at good enough starting point for the gradient descent to proceed. There are a lot of schemes to fidn good initialization values but we are going to focus on a simple general method. We draw the weights randomly from a gaussian distribution with mean zero and standard deviation sigma. The sigma value determines the order of magnitude of our outputs at the intital point of our optimizaiton. Because of the softmax on top of it, the order of magnitude also determines the peakiniess of our initial probability distribution. A large sigma will mean that our distribuiton have large peaks and going to be very opinionated. A small sigma means the our distribuiton is very uncertain about things. It is usually better to begin with an uncertain distriibution, and let the optimization become more confident as training progresses. So we use small sigma to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have every thing we traing our classifier. We got our trainign data which is normalized to have zero mean and unit variance. We multiply it by a large matrix which is initialized with random weights. We apply the softmax, then the cross entropy loss function and finally calculate the average of this loss over the entire training data.\n",
    "\n",
    "Then our optimization package computes the derivative of this loss with respect to the weights and to the biases and takes a step back into the direction opposite to that derivative. After that we start all over again. We repeat the process until we reach a minimum of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance\n",
    "\n",
    "Now that we know how to train our model there is another important part:\n",
    "We have seen that we have a training set as well as a validation set and a test set.\n",
    "This has to do with measuring with how well we are doing. Measuring performance is subtle.\n",
    "A classifier should not memorize the training set because it would fail to generalize to new input examples.\n",
    "This is not just a theoretical problem. Every classifier that we will build will try to memorize the training set. It is our job to help it to generalize to new data instead. Therefor we use a small test data that the clasifier did not see before, to test the current performance of the classifier. The problem is that training a classifier is usually trail and error. It is a cycle, we train a classifier, measure its performance and try another classifier and measure again, and again, ... we tweak the model and explore the different parameters and measure and finally we think that we have the perfect classifier and want to deploy our system and we get totally new data where we score our performance.\n",
    "\n",
    "The reason that it will probably not score that well, is that while tweaking our classifier we used our test set and therefore our classifier has seen our test data indirectly. Although just a bit of the test set after each little change in parameters, but still the test data bleeds into our training data which is what we must avoid!\n",
    "\n",
    "There are many ways to deal with this. The simplest one is splitting the data in three sets:\n",
    "\n",
    "- test set. Which we will never look at until we are done tweaking our parameters. After we made our final decision we wil use it to measure our classifiers performance.\n",
    "- training set.\n",
    "- validation set is used to measure our actual error and maybe the validation set will blend into our training set but that is ok as long as we have our test set that we can rely on is data that our classifier has not seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimizing a Logistic Classifier\n",
    "\n",
    "Training logistic regression using gradient descent is great. For one thing we are directly optimizing the error measure that we care about. That is why in practice a lot of machine learning research is about designing the right loss funciton to optimize. \n",
    "As we have expirienced our model has problems. The biggest problem is that it is very difficult to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x120e13400>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEKCAYAAADdIIPUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1162ae3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = 80\n",
    "digit_feature_vec = train_features[idx]\n",
    "digit_label = train_labels[idx]\n",
    "\n",
    "image = digit_feature_vec.reshape((28,28))\n",
    "plt.imshow(image)\n",
    "plt.xlabel(digit_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46339864  0.5286476   0.00795373]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    # initialize all TensorFlow variables\n",
    "    session.run(init)\n",
    "\n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    output = session.run(prediction, feed_dict={features: [digit_feature_vec]})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Assuming the loss function needs n floating points operations to compute. The contained gradient calculation three times as much and we consider all training data we get a lot of computations for only one single gradient descent step. Usually a gradient descent step can take 10 or 100 of times to find the minimum.\n",
    "\n",
    "To avoid this computations, we are taking a shortcut by computing an estimate of it. Although this estimate will be very bad it works quite well becasue we are going to improve it.\n",
    "\n",
    "We are going to compute the average loss for a very small random fraciton of the training set. It is important though to pick a random training data sample each time otherwise this method won't work.\n",
    "\n",
    "1. The first step is to take a small random sample of the training data\n",
    "2. Then we compute the loss\n",
    "3. After that we have found an estimate of the gradients and we pretend that this is the correct gradient.\n",
    "\n",
    "Although this won't be the correct directions we compensate for this by repeating those three steps many times.\n",
    "This will lead to very small steps but is much more efficent than doing regular gradient descent.\n",
    "\n",
    "This method is called stochastic gradient descent and scales well. Both with data and model size.\n",
    "And we want both: Big data and big models.\n",
    "\n",
    "SGD is nice and scalable but it is fundamentally a bad optimizer that happens to be the only one that is fast enough it comes with a lot of issues in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum And Learning Rate Decay\n",
    "\n",
    "To help SGD we already saw some tricks:\n",
    "\n",
    "- zero mean\n",
    "- small and equal variance\n",
    "\n",
    "These are very important for SGD. \n",
    "It is also important to initialize with random weights that have very small variance and zero mean too.\n",
    "\n",
    "Here are some other tips to consider:\n",
    "\n",
    "1. **Momentum:**\n",
    "\n",
    "Remember that at each step we are taking a very small step in a random direciton. But on aggregate those steps take us towards the minimum of the loss. We can take advantage of the knowledge that we accumuluated from pervious steps about where we should be headed. A cheap way to do that is to keep a running average of the gradient and to use that average instead of the current direction of the current batch of the data. This momentum technique works very well and often leads to faster convergence.\n",
    "\n",
    "2. **Learning Rate Decay:**\n",
    "\n",
    "Remember when replacing gradient descent with SGD we learnt that we were going to take smaller noisier steps towards our objective. How small shoudl that step be is a whole area of research as well.\n",
    "One thing that is always the case however, is that it is always beneficial to make that step smaller and smaller as we train our model. Some like to applay and exponential decay to the learning rate. Some like to make it smaller every time the loss reaches a plateau. There are lots of ways to go about it but lowering it over time is the key thing to remember."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Hyperspace\n",
    "\n",
    "Learning rate tuning can be very strange. A higher learning rate does not neccessary mean that we learn more or faster. Often we can take a lower learning rate and get to a lower loss faster.\n",
    "\n",
    "### SGD Blackmagic\n",
    "\n",
    "SGD has many hyperparameters:\n",
    "\n",
    "- Initial learing rate\n",
    "- Learning rate decay\n",
    "- Momentum\n",
    "- Batch size\n",
    "- Weight initialization\n",
    "\n",
    "It is not that hard to choose appropriate values but one thing to remember when things wont work:\n",
    "\n",
    "**Always try to lower the learning rate first.**\n",
    "\n",
    "There are lots of good solutions for small models but sadly non that is completely satisfactory so far for the very large models that we really care about. \n",
    "\n",
    "One approach that makes things a little bit easier is **ADAGRAD** which is a modification of SGD. ADAGRAD does momentum and learning rate decay implicitly for us. Using ADAGRAD makes learning often less sensitive to hyperparamters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini batch\n",
    "\n",
    "In this section, we'll go over what mini-batching is and how to apply it in TensorFlow.\n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since we can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, we train the network weights with gradient descent. Since these batches are random, we're performing SGD with each batch.\n",
    "\n",
    "Let's look at the MNIST dataset with weights and a bias to see if our machine can handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features.shape (55000, 784)\n",
      "train_labels.shape (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_features.shape\", train_features.shape)\n",
    "print(\"train_labels.shape\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the memory size of train_features, train_labels, weights, and bias in bytes. Ignore memory for overhead, just calculate the memory required for the stored data.\n",
    "\n",
    "A float32 requires 32 bit which are 4 bytes of memory, see also this [link](https://en.wikipedia.org/wiki/Single-precision_floating-point_format).\n",
    "\n",
    "train_features Shape: (55000, 784) Type: float32\n",
    "\n",
    "55000*784*4 = 172480000\n",
    "\n",
    "train_labels Shape: (55000, 10) Type: float32\n",
    "\n",
    "55000*10*4 = 2200000\n",
    "\n",
    "weights Shape: (784, 10) Type: float32\n",
    "\n",
    "784*10*4 = 31360\n",
    "\n",
    "bias Shape: (10,) Type: float32\n",
    "\n",
    "10*4 = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total memory space required for the inputs, weights and bias is around 174 megabytes, which isn't that much memory. We could train this whole dataset on most CPUs and GPUs.\n",
    "\n",
    "But larger datasets that we'll use in the future measured in gigabytes or more. It's possible to purchase more memory, but it's expensive. A Titan X GPU with 12 GB of memory costs over $1,000.\n",
    "\n",
    "Instead, in order to run large models on our machines, we'll see how to use mini-batching.\n",
    "\n",
    "Let's look at how we implement mini-batching in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Mini-batching\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to use mini-batching, we must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine we'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, we'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "\n",
    "In that case, the size of the batches would vary, so we need to take advantage of TensorFlow's [`tf.placeholder()`](https://www.tensorflow.org/api_docs/python/tf/placeholder) function to receive the varying batch sizes.\n",
    "\n",
    "Continuing the example, if each sample had `n_input = 784` features and `n_classes = 10` possible labels, the dimensions for features would be `[None, n_input]` and labels would be `[None, n_classes]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does None do here?\n",
    "\n",
    "The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than `0`.\n",
    "\n",
    "Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of `128` samples or the single batch of `104` samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the parameters below, how many batches are there, and what is the last batch size?\n",
    "\n",
    "features is (50000, 400)\n",
    "\n",
    "labels is (50000, 10)\n",
    "\n",
    "batch_size is 128\n",
    "\n",
    "How many batches are there?\n",
    "391 (390*128 + 1)\n",
    "\n",
    "What is the last batch size?\n",
    "80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the `batches function` to batch `features` and `labels`. The function returns each batch with a maximum size of `batch_size`. To help you with the quiz, look at the following example output of a working batches function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`example_batches = batches(3, example_features, example_labels)`\n",
    "\n",
    "The `example_batches` variable would be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['F11', 'F12', 'F13', 'F14'],\n",
       "   ['F21', 'F22', 'F23', 'F24'],\n",
       "   ['F31', 'F32', 'F33', 'F34']],\n",
       "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
       " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    # 2 batches:\n",
    "    #   First is a batch of size 3.\n",
    "    #   Second is a batch of size 1\n",
    "    [\n",
    "        # First Batch is size 3\n",
    "        [\n",
    "            # 3 samples of features.\n",
    "            # There are 4 features per sample.\n",
    "            ['F11', 'F12', 'F13', 'F14'],\n",
    "            ['F21', 'F22', 'F23', 'F24'],\n",
    "            ['F31', 'F32', 'F33', 'F34']\n",
    "        ], [\n",
    "            # 3 samples of labels.\n",
    "            # There are 2 labels per sample.\n",
    "            ['L11', 'L12'],\n",
    "            ['L21', 'L22'],\n",
    "            ['L31', 'L32']\n",
    "        ]\n",
    "    ], [\n",
    "        # Second Batch is size 1.\n",
    "        # Since batch size is 3, there is only one sample left from the 4 samples.\n",
    "        [\n",
    "            # 1 sample of features.\n",
    "            ['F41', 'F42', 'F43', 'F44']\n",
    "        ], [\n",
    "            # 1 sample of labels.\n",
    "            ['L41', 'L42']\n",
    "        ]\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # Implement batching\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['F11', 'F12', 'F13', 'F14'],\n",
       "   ['F21', 'F22', 'F23', 'F24'],\n",
       "   ['F31', 'F32', 'F33', 'F34']],\n",
       "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
       " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batches = batches(3, example_features, example_labels)\n",
    "example_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use mini-batching to feed batches of MNIST features and labels into a linear model.\n",
    "\n",
    "Set the batch size and run the optimizer over all the batches with the batches function. The recommended batch size is `128`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "Test Accuracy: 0.10029999911785126\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is low, but you probably know that you could train on the dataset more than once. You can train a model using the dataset multiple times. We'll go over this subject in the next section where we talk about \"epochs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs\n",
    "\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs.\n",
    "\n",
    "The following TensorFlow code trains a model using 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 10.7     Valid Accuracy: 0.0814\n",
      "Epoch: 1    - Cost: 9.98     Valid Accuracy: 0.091\n",
      "Epoch: 2    - Cost: 9.32     Valid Accuracy: 0.101\n",
      "Epoch: 3    - Cost: 8.76     Valid Accuracy: 0.114\n",
      "Epoch: 4    - Cost: 8.29     Valid Accuracy: 0.131\n",
      "Epoch: 5    - Cost: 7.87     Valid Accuracy: 0.151\n",
      "Epoch: 6    - Cost: 7.49     Valid Accuracy: 0.168\n",
      "Epoch: 7    - Cost: 7.15     Valid Accuracy: 0.186\n",
      "Epoch: 8    - Cost: 6.83     Valid Accuracy: 0.201\n",
      "Epoch: 9    - Cost: 6.54     Valid Accuracy: 0.219\n",
      "Epoch: 10   - Cost: 6.26     Valid Accuracy: 0.241\n",
      "Epoch: 11   - Cost: 6.0      Valid Accuracy: 0.257\n",
      "Epoch: 12   - Cost: 5.76     Valid Accuracy: 0.273\n",
      "Epoch: 13   - Cost: 5.54     Valid Accuracy: 0.29 \n",
      "Epoch: 14   - Cost: 5.33     Valid Accuracy: 0.303\n",
      "Epoch: 15   - Cost: 5.13     Valid Accuracy: 0.32 \n",
      "Epoch: 16   - Cost: 4.95     Valid Accuracy: 0.337\n",
      "Epoch: 17   - Cost: 4.78     Valid Accuracy: 0.349\n",
      "Epoch: 18   - Cost: 4.62     Valid Accuracy: 0.362\n",
      "Epoch: 19   - Cost: 4.47     Valid Accuracy: 0.372\n",
      "Epoch: 20   - Cost: 4.33     Valid Accuracy: 0.384\n",
      "Epoch: 21   - Cost: 4.2      Valid Accuracy: 0.397\n",
      "Epoch: 22   - Cost: 4.08     Valid Accuracy: 0.408\n",
      "Epoch: 23   - Cost: 3.97     Valid Accuracy: 0.418\n",
      "Epoch: 24   - Cost: 3.86     Valid Accuracy: 0.428\n",
      "Epoch: 25   - Cost: 3.76     Valid Accuracy: 0.437\n",
      "Epoch: 26   - Cost: 3.67     Valid Accuracy: 0.446\n",
      "Epoch: 27   - Cost: 3.58     Valid Accuracy: 0.455\n",
      "Epoch: 28   - Cost: 3.49     Valid Accuracy: 0.465\n",
      "Epoch: 29   - Cost: 3.41     Valid Accuracy: 0.473\n",
      "Epoch: 30   - Cost: 3.34     Valid Accuracy: 0.484\n",
      "Epoch: 31   - Cost: 3.27     Valid Accuracy: 0.491\n",
      "Epoch: 32   - Cost: 3.2      Valid Accuracy: 0.499\n",
      "Epoch: 33   - Cost: 3.14     Valid Accuracy: 0.507\n",
      "Epoch: 34   - Cost: 3.07     Valid Accuracy: 0.513\n",
      "Epoch: 35   - Cost: 3.02     Valid Accuracy: 0.521\n",
      "Epoch: 36   - Cost: 2.96     Valid Accuracy: 0.527\n",
      "Epoch: 37   - Cost: 2.91     Valid Accuracy: 0.535\n",
      "Epoch: 38   - Cost: 2.86     Valid Accuracy: 0.54 \n",
      "Epoch: 39   - Cost: 2.81     Valid Accuracy: 0.548\n",
      "Epoch: 40   - Cost: 2.76     Valid Accuracy: 0.555\n",
      "Epoch: 41   - Cost: 2.72     Valid Accuracy: 0.561\n",
      "Epoch: 42   - Cost: 2.67     Valid Accuracy: 0.568\n",
      "Epoch: 43   - Cost: 2.63     Valid Accuracy: 0.574\n",
      "Epoch: 44   - Cost: 2.59     Valid Accuracy: 0.58 \n",
      "Epoch: 45   - Cost: 2.56     Valid Accuracy: 0.584\n",
      "Epoch: 46   - Cost: 2.52     Valid Accuracy: 0.588\n",
      "Epoch: 47   - Cost: 2.48     Valid Accuracy: 0.595\n",
      "Epoch: 48   - Cost: 2.45     Valid Accuracy: 0.601\n",
      "Epoch: 49   - Cost: 2.42     Valid Accuracy: 0.606\n",
      "Epoch: 50   - Cost: 2.38     Valid Accuracy: 0.611\n",
      "Epoch: 51   - Cost: 2.35     Valid Accuracy: 0.615\n",
      "Epoch: 52   - Cost: 2.32     Valid Accuracy: 0.618\n",
      "Epoch: 53   - Cost: 2.29     Valid Accuracy: 0.621\n",
      "Epoch: 54   - Cost: 2.26     Valid Accuracy: 0.622\n",
      "Epoch: 55   - Cost: 2.24     Valid Accuracy: 0.625\n",
      "Epoch: 56   - Cost: 2.21     Valid Accuracy: 0.628\n",
      "Epoch: 57   - Cost: 2.18     Valid Accuracy: 0.633\n",
      "Epoch: 58   - Cost: 2.16     Valid Accuracy: 0.637\n",
      "Epoch: 59   - Cost: 2.13     Valid Accuracy: 0.64 \n",
      "Epoch: 60   - Cost: 2.11     Valid Accuracy: 0.642\n",
      "Epoch: 61   - Cost: 2.09     Valid Accuracy: 0.644\n",
      "Epoch: 62   - Cost: 2.06     Valid Accuracy: 0.648\n",
      "Epoch: 63   - Cost: 2.04     Valid Accuracy: 0.653\n",
      "Epoch: 64   - Cost: 2.02     Valid Accuracy: 0.655\n",
      "Epoch: 65   - Cost: 2.0      Valid Accuracy: 0.659\n",
      "Epoch: 66   - Cost: 1.98     Valid Accuracy: 0.663\n",
      "Epoch: 67   - Cost: 1.96     Valid Accuracy: 0.666\n",
      "Epoch: 68   - Cost: 1.94     Valid Accuracy: 0.668\n",
      "Epoch: 69   - Cost: 1.92     Valid Accuracy: 0.671\n",
      "Epoch: 70   - Cost: 1.9      Valid Accuracy: 0.673\n",
      "Epoch: 71   - Cost: 1.88     Valid Accuracy: 0.674\n",
      "Epoch: 72   - Cost: 1.86     Valid Accuracy: 0.676\n",
      "Epoch: 73   - Cost: 1.85     Valid Accuracy: 0.678\n",
      "Epoch: 74   - Cost: 1.83     Valid Accuracy: 0.68 \n",
      "Epoch: 75   - Cost: 1.81     Valid Accuracy: 0.682\n",
      "Epoch: 76   - Cost: 1.8      Valid Accuracy: 0.684\n",
      "Epoch: 77   - Cost: 1.78     Valid Accuracy: 0.687\n",
      "Epoch: 78   - Cost: 1.77     Valid Accuracy: 0.69 \n",
      "Epoch: 79   - Cost: 1.75     Valid Accuracy: 0.692\n",
      "Test Accuracy: 0.6929000020027161\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 80\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch: 0    - Cost: 15.3     Valid Accuracy: 0.12 \n",
    "Epoch: 1    - Cost: 13.7     Valid Accuracy: 0.135\n",
    "Epoch: 2    - Cost: 12.6     Valid Accuracy: 0.149\n",
    "Epoch: 3    - Cost: 11.9     Valid Accuracy: 0.16 \n",
    "Epoch: 4    - Cost: 11.2     Valid Accuracy: 0.168\n",
    "Epoch: 5    - Cost: 10.7     Valid Accuracy: 0.18 \n",
    "Epoch: 6    - Cost: 10.2     Valid Accuracy: 0.19 \n",
    "Epoch: 7    - Cost: 9.73     Valid Accuracy: 0.205\n",
    "Epoch: 8    - Cost: 9.31     Valid Accuracy: 0.214\n",
    "Epoch: 9    - Cost: 8.93     Valid Accuracy: 0.227\n",
    "Test Accuracy: 0.23890000581741333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each epoch attempts to move to a lower cost, leading to better accuracy.\n",
    "\n",
    "This model continues to improve accuracy up to Epoch 9. Let's increase the number of epochs to 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch: 0    - Cost: 11.2     Valid Accuracy: 0.0864\n",
    "Epoch: 1    - Cost: 10.3     Valid Accuracy: 0.0854\n",
    "Epoch: 2    - Cost: 9.65     Valid Accuracy: 0.0898\n",
    "Epoch: 3    - Cost: 9.22     Valid Accuracy: 0.0988\n",
    "Epoch: 4    - Cost: 8.87     Valid Accuracy: 0.112\n",
    "Epoch: 5    - Cost: 8.56     Valid Accuracy: 0.131\n",
    "Epoch: 6    - Cost: 8.26     Valid Accuracy: 0.152\n",
    "Epoch: 7    - Cost: 7.98     Valid Accuracy: 0.167\n",
    "Epoch: 8    - Cost: 7.7      Valid Accuracy: 0.186\n",
    "...\n",
    "Epoch: 20   - Cost: 5.37     Valid Accuracy: 0.375\n",
    "Epoch: 21   - Cost: 5.24     Valid Accuracy: 0.39 \n",
    "Epoch: 22   - Cost: 5.11     Valid Accuracy: 0.399\n",
    "Epoch: 23   - Cost: 4.99     Valid Accuracy: 0.41 \n",
    "Epoch: 24   - Cost: 4.88     Valid Accuracy: 0.419\n",
    "\n",
    "...\n",
    "Epoch: 83   - Cost: 2.17     Valid Accuracy: 0.694\n",
    "Epoch: 84   - Cost: 2.15     Valid Accuracy: 0.696\n",
    "Epoch: 85   - Cost: 2.13     Valid Accuracy: 0.697\n",
    "Epoch: 86   - Cost: 2.11     Valid Accuracy: 0.699\n",
    "Epoch: 87   - Cost: 2.09     Valid Accuracy: 0.701\n",
    "Epoch: 88   - Cost: 2.08     Valid Accuracy: 0.702\n",
    "Epoch: 89   - Cost: 2.06     Valid Accuracy: 0.703\n",
    "...\n",
    "Epoch: 97   - Cost: 1.94     Valid Accuracy: 0.718\n",
    "Epoch: 98   - Cost: 1.92     Valid Accuracy: 0.719\n",
    "Epoch: 99   - Cost: 1.91     Valid Accuracy: 0.721\n",
    "Test Accuracy: 0.7197999954223633\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the output above, you can see the model doesn't increase the validation accuracy after epoch 80. Let's see what happens when we increase the learning rate.\n",
    "\n",
    "`learn_rate = 0.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch: 0    - Cost: 1.48     Valid Accuracy: 0.713\n",
    "Epoch: 1    - Cost: 1.04     Valid Accuracy: 0.795\n",
    "Epoch: 2    - Cost: 0.89     Valid Accuracy: 0.825\n",
    "Epoch: 3    - Cost: 0.809    Valid Accuracy: 0.841\n",
    "Epoch: 4    - Cost: 0.756    Valid Accuracy: 0.853\n",
    "Epoch: 5    - Cost: 0.719    Valid Accuracy: 0.861\n",
    "...\n",
    "Epoch: 21   - Cost: 0.514    Valid Accuracy: 0.899\n",
    "Epoch: 22   - Cost: 0.507    Valid Accuracy: 0.899\n",
    "Epoch: 23   - Cost: 0.5      Valid Accuracy: 0.9  \n",
    "Epoch: 24   - Cost: 0.494    Valid Accuracy: 0.901\n",
    "Epoch: 25   - Cost: 0.487    Valid Accuracy: 0.901\n",
    "Epoch: 26   - Cost: 0.482    Valid Accuracy: 0.901\n",
    "Epoch: 27   - Cost: 0.476    Valid Accuracy: 0.902\n",
    "Epoch: 28   - Cost: 0.471    Valid Accuracy: 0.902\n",
    "...\n",
    "Epoch: 50   - Cost: 0.392    Valid Accuracy: 0.91 \n",
    "Epoch: 51   - Cost: 0.389    Valid Accuracy: 0.91 \n",
    "Epoch: 52   - Cost: 0.387    Valid Accuracy: 0.91 \n",
    "Epoch: 53   - Cost: 0.384    Valid Accuracy: 0.91 \n",
    "Epoch: 54   - Cost: 0.382    Valid Accuracy: 0.91 \n",
    "Epoch: 55   - Cost: 0.38     Valid Accuracy: 0.91 \n",
    "Epoch: 56   - Cost: 0.378    Valid Accuracy: 0.911\n",
    "Epoch: 57   - Cost: 0.375    Valid Accuracy: 0.911\n",
    "Epoch: 58   - Cost: 0.373    Valid Accuracy: 0.911\n",
    "Epoch: 59   - Cost: 0.371    Valid Accuracy: 0.911\n",
    "Epoch: 60   - Cost: 0.369    Valid Accuracy: 0.912\n",
    "Epoch: 61   - Cost: 0.367    Valid Accuracy: 0.912\n",
    "Epoch: 62   - Cost: 0.365    Valid Accuracy: 0.912\n",
    "Epoch: 63   - Cost: 0.363    Valid Accuracy: 0.912\n",
    "Epoch: 64   - Cost: 0.361    Valid Accuracy: 0.912\n",
    "Epoch: 65   - Cost: 0.359    Valid Accuracy: 0.913\n",
    "...\n",
    "Epoch: 88   - Cost: 0.322    Valid Accuracy: 0.915\n",
    "Epoch: 89   - Cost: 0.321    Valid Accuracy: 0.915\n",
    "Epoch: 90   - Cost: 0.32     Valid Accuracy: 0.915\n",
    "Epoch: 91   - Cost: 0.318    Valid Accuracy: 0.915\n",
    "Epoch: 92   - Cost: 0.317    Valid Accuracy: 0.915\n",
    "Epoch: 93   - Cost: 0.316    Valid Accuracy: 0.915\n",
    "Epoch: 94   - Cost: 0.314    Valid Accuracy: 0.915\n",
    "Epoch: 95   - Cost: 0.313    Valid Accuracy: 0.915\n",
    "Epoch: 96   - Cost: 0.312    Valid Accuracy: 0.915\n",
    "Epoch: 97   - Cost: 0.311    Valid Accuracy: 0.915\n",
    "Epoch: 98   - Cost: 0.309    Valid Accuracy: 0.915\n",
    "Epoch: 99   - Cost: 0.308    Valid Accuracy: 0.915\n",
    "Test Accuracy: 0.9133999943733215\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the learning rate was increased too much. The final accuracy was lower, and it stopped improving earlier. Let's stick with the previous learning rate, but change the number of epochs to 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch: 0    - Cost: 10.7     Valid Accuracy: 0.0814\n",
    "Epoch: 1    - Cost: 9.98     Valid Accuracy: 0.091\n",
    "Epoch: 2    - Cost: 9.32     Valid Accuracy: 0.101\n",
    "Epoch: 3    - Cost: 8.76     Valid Accuracy: 0.114\n",
    "Epoch: 4    - Cost: 8.29     Valid Accuracy: 0.131\n",
    "Epoch: 5    - Cost: 7.87     Valid Accuracy: 0.151\n",
    "Epoch: 6    - Cost: 7.49     Valid Accuracy: 0.168\n",
    "Epoch: 7    - Cost: 7.15     Valid Accuracy: 0.186\n",
    "Epoch: 8    - Cost: 6.83     Valid Accuracy: 0.201\n",
    "...\n",
    "Epoch: 35   - Cost: 3.02     Valid Accuracy: 0.521\n",
    "Epoch: 36   - Cost: 2.96     Valid Accuracy: 0.527\n",
    "Epoch: 37   - Cost: 2.91     Valid Accuracy: 0.535\n",
    "Epoch: 38   - Cost: 2.86     Valid Accuracy: 0.54 \n",
    "...\n",
    "Epoch: 70   - Cost: 1.9      Valid Accuracy: 0.673\n",
    "Epoch: 71   - Cost: 1.88     Valid Accuracy: 0.674\n",
    "Epoch: 72   - Cost: 1.86     Valid Accuracy: 0.676\n",
    "Epoch: 73   - Cost: 1.85     Valid Accuracy: 0.678\n",
    "Epoch: 74   - Cost: 1.83     Valid Accuracy: 0.68 \n",
    "Epoch: 75   - Cost: 1.81     Valid Accuracy: 0.682\n",
    "Epoch: 76   - Cost: 1.8      Valid Accuracy: 0.684\n",
    "Epoch: 77   - Cost: 1.78     Valid Accuracy: 0.687\n",
    "Epoch: 78   - Cost: 1.77     Valid Accuracy: 0.69 \n",
    "Epoch: 79   - Cost: 1.75     Valid Accuracy: 0.692\n",
    "Test Accuracy: 0.6929000020027161\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy only reached 0.86, but that could be because the learning rate was too high. Lowering the learning rate would require more epochs, but could ultimately achieve better accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
